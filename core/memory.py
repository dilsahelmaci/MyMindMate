# -*- coding: utf-8 -*-
"""
Yapay Zeka Uzun S羹reli Haf覺za Y繹netimi Mod羹l羹 (Pinecone Entegrasyonu).

Bu mod羹l, yapay zekan覺n kullan覺c覺ya 繹zel uzun s羹reli haf覺zas覺n覺 y繹netir.
Kullan覺c覺yla yap覺lan 繹nemli konumalar覺, g羹nl羹kleri ve hedefleri anlamsal
vekt繹rlere d繹n羹t羹rerek Pinecone adl覺 vekt繹r veritaban覺nda saklar. Bu sayede
yapay zeka, ge癟mi konumalar覺 "hat覺rlayabilir" ve daha balamsal yan覺tlar
羹retebilir (Retrieval-Augmented Generation - RAG).

襤leyi:
1.  Metinler, `get_gemini_embedding` ile say覺sal vekt繹rlere 癟evrilir.
2.  Bu vekt繹rler, kullan覺c覺 ID'si ile etiketlenerek Pinecone'a kaydedilir (`save_to_memory`).
3.  Yeni bir sohbette, kullan覺c覺n覺n mesaj覺na anlamsal olarak en yak覺n ge癟mi
    konumalar Pinecone'dan aran覺r (`search_memory`).
4.  Bulunan bu "hat覺ralar", yapay zekaya ek balam olarak sunulur.
"""
import streamlit as st
import numpy as np
from pinecone import Pinecone, ServerlessSpec
import google.generativeai as genai
from typing import List, Dict, Any, Optional
import uuid

# --- GVENL襤 KONF襤GRASYON VE BALATMA ---

try:
    PINECONE_API_KEY = st.secrets["pinecone"]["api_key"]
    GOOGLE_API_KEY = st.secrets["google"]["api_key"]
    genai.configure(api_key=GOOGLE_API_KEY)
except (KeyError, AttributeError):
    raise RuntimeError(
        "Pinecone veya Google API anahtar覺 bulunamad覺. "
        "L羹tfen `.streamlit/secrets.toml` dosyan覺z覺 kontrol edin."
    )

# --- PINECONE INDEX KURULUMU ---

EMBED_DIM = 768  # Gemini 'embedding-001' modelinin vekt繹r boyutu.
INDEX_NAME = "mymindmate-memory"  # Pinecone'daki index'imizin ad覺.

pc = Pinecone(api_key=PINECONE_API_KEY)

# Uygulama ilk kez 癟al覺t覺r覺ld覺覺nda, Pinecone'da index'imiz yoksa olutur.
# Bu, manuel kurulum ihtiyac覺n覺 ortadan kald覺r覺r.
if INDEX_NAME not in pc.list_indexes().names():
    pc.create_index(
        name=INDEX_NAME,
        dimension=EMBED_DIM,
        metric="cosine",  # Anlamsal benzerlik i癟in en yayg覺n ve etkili metrik.
        spec=ServerlessSpec(
            cloud="aws",
            region="us-east-1"  # Pinecone 羹cretsiz katman覺n覺n standart b繹lgesi.
        )
    )

# Index'e balanarak 羹zerinde ilem yapmaya haz覺r hale gel.
pinecone_index = pc.Index(INDEX_NAME)

# --- YARDIMCI FONKS襤YON: EMBEDDING ALMA ---

@st.cache_data(show_spinner=False)
def get_gemini_embedding(text: str) -> List[float]:
    """
    Verilen metin i癟in Google Gemini embedding'i oluturur.
    
    `@st.cache_data` sayesinde ayn覺 metin i癟in tekrar tekrar API 癟ar覺s覺
    yap覺lmaz, bu da performans覺 art覺r覺r ve maliyeti d羹羹r羹r.
    """
    try:
        response = genai.embed_content(
            model="models/embedding-001",
            content=text,
            task_type="retrieval_document"  # Arama ama癟l覺 embedding olduunu belirtir.
        )
        return response["embedding"]
    except Exception as e:
        st.error(f"Embedding al覺n覺rken bir hata olutu: {e}")
        return []

# --- ANA HAFIZA YNET襤M襤 FONKS襤YONLARI ---

def save_to_memory(user_id: str, text: str, metadata: Dict[str, Any]):
    """Bir metni, meta verileriyle birlikte kullan覺c覺n覺n haf覺zas覺na kaydeder."""
    if not user_id or not text:
        return

    # Pinecone'a g繹nderilecek her vekt繹r羹n benzersiz bir ID'si olmal覺d覺r.
    vector_id = str(uuid.uuid4())
    embedding = get_gemini_embedding(text)
    if not embedding:
        return  # Embedding al覺namad覺ysa kaydetme.

    # Pinecone'a y羹klenecek vekt繹r羹 haz覺rla. Metaveri, arama sonu癟lar覺n覺
    # zenginletirmek ve filtrelemek i癟in kritik 繹neme sahiptir.
    vector_to_upsert = {
        "id": vector_id,
        "values": embedding,
        "metadata": {
            "user_id": user_id,  # Hangi kullan覺c覺ya ait olduunu belirtir.
            "text": text,        # Orijinal metni saklar.
            **metadata          # Gelen dier t羹m meta verileri ekler (繹rn: 'role', 'timestamp').
        }
    }

    try:
        pinecone_index.upsert(vectors=[vector_to_upsert])
    except Exception as e:
        st.error(f"Haf覺zaya kaydederken bir Pinecone hatas覺 olutu: {e}")

def search_memory(user_id: str, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
    """
    Kullan覺c覺n覺n haf覺zas覺nda, verilen sorguyla anlamsal olarak en ilikili
    kay覺tlar覺 arar.
    """
    if not user_id or not query:
        return []

    query_embedding = get_gemini_embedding(query)
    if not query_embedding:
        return []

    try:
        # Pinecone'da arama yap ve sonu癟lar覺 sadece ilgili kullan覺c覺 i癟in filtrele.
        results = pinecone_index.query(
            vector=query_embedding,
            top_k=top_k,  # En alakal覺 `top_k` sonucu getir.
            filter={"user_id": {"$eq": user_id}},  # Sadece bu kullan覺c覺ya ait vekt繹rleri ara.
            include_metadata=True  # Sonu癟larla birlikte meta verileri de getir.
        )
        
        # Sonu癟lar覺, sadece meta verileri i癟eren temiz bir listeye d繹n羹t羹r.
        found_matches = [match.metadata for match in results.matches] if results.matches else []
        return found_matches
    except Exception as e:
        st.error(f"Haf覺zada arama yaparken bir Pinecone hatas覺 olutu: {e}")
        return []

def delete_user_memory(user_id: str):
    """
    Belirli bir kullan覺c覺ya ait t羹m haf覺za kay覺tlar覺n覺 (vekt繹rleri) Pinecone'dan siler.
    Genellikle hesap silme ilemiyle birlikte kullan覺l覺r.
    """
    if not user_id:
        return
    
    try:
        pinecone_index.delete(filter={"user_id": {"$eq": user_id}})
        st.toast(f"{user_id} i癟in AI haf覺zas覺 baar覺yla temizlendi.", icon="")
    except Exception as e:
        st.error(f"Kullan覺c覺 haf覺zas覺n覺 silerken bir Pinecone hatas覺 olutu: {e}")